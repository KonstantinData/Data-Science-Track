{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝\n",
    "Data preparation is a crucial step in Machine Learning (ML), and understanding the different types of data is fundamental for selecting appropriate preprocessing methods and algorithms. Data types refer to the nature of the data you're working with and influence how you treat it during preparation and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Key Data Types: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric (Quantitative) Data  \n",
    "- **Continuous:** Can take any real number value within a range. Examples include weight, height, and temperature.  \n",
    "- **Discrete:** Represents countable quantities and only takes integer values. Examples include the number of products sold or the number of customers in a store.  \n",
    "\n",
    "Categorical (Qualitative) Data  \n",
    "- **Nominal:** Unordered categories or labels, such as gender (male, female), colors (red, blue, green), or product types.  \n",
    "- **Ordinal:** Ordered categories with a meaningful rank or order, such as education levels (high school, bachelor’s, master’s) or ratings (poor, fair, good, excellent).  \n",
    "\n",
    "Text Data  \n",
    "- Text data includes any type of data that consists of words, sentences, or documents, such as product reviews, social media posts, or emails.  \n",
    "\n",
    "Time Series Data  \n",
    "- This data type represents observations collected at specific time intervals, such as stock prices, weather data, or sales over time. Time series data is ordered and often exhibits patterns like trends or seasonality.  \n",
    "\n",
    "Boolean (Binary) Data  \n",
    "- Binary data consists of two possible values, often represented as 0 and 1 or True and False. Examples include yes/no questions or outcomes such as spam/not spam.  \n",
    "\n",
    "Image Data  \n",
    "- Image data consists of pixel values, typically represented as arrays or matrices. Each pixel value is either grayscale (0 to 255) or RGB (three values, one for each channel: Red, Green, Blue). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Age': [25, np.nan, 35, 45, np.nan, 50],\n",
    "    'Salary': [50000, 60000, np.nan, 80000, 90000, np.nan],\n",
    "    'City': ['New York', 'Los Angeles', 'New York', np.nan, \n",
    "'San Francisco', 'Chicago']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us:\n",
    "\n",
    "- 2 missing values in Age\n",
    "- 2 missing values in Salary\n",
    "- 1 missing value in City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding the Impact on Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using this dataset for machine learning, missing values can:\n",
    "\n",
    "- Cause errors in models (most ML algorithms do not handle NaNs).\n",
    "- Bias results if missing values are not random.\n",
    "- Reduce training data if rows are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Selecting only numerical features for a simple regression model\n",
    "X = df[['Age']]\n",
    "y = df['Salary']\n",
    "\n",
    "# Trying to fit a model with missing values (this will fail)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)  # Will raise an error due to NaN values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔴 ValueError: Input X contains NaN. LinearRegression does not accept missing values encoded as NaN natively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to decide on a strategy before modeling.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Dropping Missing Values (Not Always Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped =df.dropna()\n",
    "print(df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This removes rows with NaNs but reduces dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - Filling Numerical Data with Mean/Median & Categorical Data with Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the dataset\n",
    "print(\"Original Data:\\n\", df)\n",
    "\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())  # Mean imputation\n",
    "df['Salary'] = df['Salary'].fillna(df['Salary'].median())  # Median imputation\n",
    "\n",
    "# Task 2: Fill missing categorical values with the mode\n",
    "df['City'] = df['City'].fillna(df['City'].mode()[0])\n",
    "\n",
    "# Display the modified dataset\n",
    "print(\"\\nData after filling missing values:\\n\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **mean/median**\n",
    "- Mean: Suitable for normally distributed data.<br>\n",
    "- Median: Better for skewed data (e.g., salaries often have outliers).\n",
    "<br>\n",
    "\n",
    "> **mode**\n",
    "- Mode: Uses the most frequent category to fill missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Should You Still Consider Mean/Median?\n",
    "\n",
    "🔹 **Small datasets** → If you don’t have enough data, KNNImputer might not work well.  \n",
    "🔹 **Outliers** → If data has extreme values, mean/median imputation might be more stable.  \n",
    "🔹 **Computational efficiency** → KNNImputer is more computationally expensive than mean/median.  \n",
    "\n",
    "For most real-world structured datasets, your choice of KNN imputation is the preferred method. 🚀  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Using Machine Learning for Imputation **(Numerical Data)** + Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of simple mean/median, we can predict missing values using another model.\n",
    "\n",
    "Example: Predict missing Salary using Age:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚀 Best Practice (Using KNNImputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "     Age   Salary           City\n",
      "0  25.0  50000.0       New York\n",
      "1   NaN  60000.0    Los Angeles\n",
      "2  35.0      NaN       New York\n",
      "3  45.0  80000.0            NaN\n",
      "4   NaN  90000.0  San Francisco\n",
      "5  50.0      NaN        Chicago\n",
      "\n",
      "Data after filling missing values:\n",
      "     Age   Salary           City\n",
      "0  25.0  50000.0       New York\n",
      "1  35.0  60000.0    Los Angeles\n",
      "2  35.0  65000.0       New York\n",
      "3  45.0  80000.0       New York\n",
      "4  35.0  90000.0  San Francisco\n",
      "5  50.0  65000.0        Chicago\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Age': [25, np.nan, 35, 45, np.nan, 50],\n",
    "    'Salary': [50000, 60000, np.nan, 80000, 90000, np.nan],\n",
    "    'City': ['New York', 'Los Angeles', 'New York', np.nan, 'San Francisco', 'Chicago']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the dataset before processing\n",
    "print(\"Original Data:\\n\", df)\n",
    "\n",
    "# Step 1: Apply KNN Imputer for numerical columns\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df[['Age', 'Salary']] = imputer.fit_transform(df[['Age', 'Salary']])\n",
    "\n",
    "# Step 2: Fill missing categorical values with mode\n",
    "df['City'] = df['City'].fillna(df['City'].mode()[0])\n",
    "\n",
    "# Display the modified dataset\n",
    "print(\"\\nData after filling missing values:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Why is KNNImputer Better Than Mean/Median Imputation?***\n",
    "\n",
    "✔ **More accurate predictions** → Instead of replacing missing values with a fixed number (mean/median), KNNImputer estimates values based on similar existing data.  \n",
    "✔ **Handles complex relationships** → If Age and Salary have patterns, KNNImputer preserves those relationships.  \n",
    "✔ **Reduces bias** → Mean/median imputation assumes a normal distribution, which may not always be the case.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Scale Numerical Features Using StandardScaler  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After handling missing values, numerical features should be standardized to ensure consistent scaling. **StandardScaler** transforms data so that it has a **mean of 0** and a **standard deviation of 1**, improving model performance for algorithms sensitive to feature magnitude.  \n",
    "\n",
    "**Steps to Apply StandardScaler**  \n",
    "\n",
    "1. **Identify numerical features** → Select columns that require scaling.  \n",
    "2. **Apply StandardScaler** → Fit and transform the numerical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after scaling:\n",
      "         Age   Salary           City\n",
      "0 -1.754575 -1.46385       New York\n",
      "1 -0.250654 -0.87831    Los Angeles\n",
      "2 -0.501307  1.46385       New York\n",
      "3  0.751961  0.29277       New York\n",
      "4  0.375980  0.87831  San Francisco\n",
      "5  1.378595 -0.29277        Chicago\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Age': [25, 37, 35, 45, 42, 50],\n",
    "    'Salary': [50000, 60000, 100000, 80000, 90000, 70000],\n",
    "    'City': ['New York', 'Los Angeles', 'New York', 'New York', 'San Francisco', 'Chicago']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = ['Age', 'Salary']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "print(\"\\nData after scaling:\\n\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applied StandardScaler for Feature Scaling**\n",
    "\n",
    "- `from sklearn.preprocessing import StandardScaler` → Imports the correct scaler.  \n",
    "- `scaler = StandardScaler()` → Initializes the StandardScaler.  \n",
    "- `df[numeric_cols] = scaler.fit_transform(df[numeric_cols])` → Standardizes the numerical columns.  \n",
    "\n",
    "This transforms **Age** and **Salary** so that they have a **mean of 0** and a **standard deviation of 1**, ensuring consistency for machine learning models. 🚀  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Explanation of StandardScaler Transformation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply **StandardScaler**, it transforms numerical features like **Age** and **Salary** so that they have:  \n",
    "\n",
    "✔ **Mean of 0** → The average value of the feature becomes **0** after scaling.  \n",
    "✔ **Standard deviation of 1** → The spread (variance) of the feature is normalized to **1**.  \n",
    "\n",
    "#### **Formula for Standardization**  \n",
    "Each value \\( x \\) in the column is transformed using the formula:  \n",
    "\n",
    "\\[\n",
    "x' = \\frac{x - \\mu}{\\sigma}\n",
    "\\]\n",
    "\n",
    "where:  \n",
    "- \\( x' \\) = Transformed (standardized) value  \n",
    "- \\( x \\) = Original value  \n",
    "- \\( \\mu \\) = Mean of the column  \n",
    "- \\( \\sigma \\) = Standard deviation of the column  \n",
    "\n",
    "This ensures that **all features are on the same scale**, preventing any one feature (e.g., Salary with large values) from dominating the model.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Example: Before and After Scaling**  \n",
    "\n",
    "##### **Original Data:**  \n",
    "| Age  | Salary  |  \n",
    "|------|--------|  \n",
    "| 25   | 50000  |  \n",
    "| 37   | 60000  |  \n",
    "| 35   | 100000 |  \n",
    "| 45   | 80000  |  \n",
    "| 42   | 90000  |  \n",
    "| 50   | 70000  |  \n",
    "\n",
    "##### **After StandardScaler Transformation:**  \n",
    "| Age (scaled) | Salary (scaled) |  \n",
    "|-------------|---------------|  \n",
    "| -1.5       | -1.2          |  \n",
    "| -0.5       | -0.8          |  \n",
    "| -0.7       | 1.9           |  \n",
    "| 0.5        | 0.3           |  \n",
    "| 0.2        | 0.9           |  \n",
    "| 1.0        | -0.1          |  \n",
    "\n",
    "- **All values are now centered around 0.**  \n",
    "- **The range is adjusted so that variance is standardized.**  \n",
    "- **Machine learning models perform better when features are scaled consistently.**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - ### Text Preprocessing: Lowercasing and Removing Punctuation  \n",
    "\n",
    "In text-based machine learning tasks, preprocessing is essential to clean and standardize textual data. This step ensures consistency and removes unnecessary elements such as punctuation and special characters, making the data more suitable for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello! how's it going? i'm excited.\n",
      "hello hows it going im excited\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "text = \"Hello! How's it going? I'm excited.\"\n",
    "\n",
    "# Task 1: Convert to lowercase\n",
    "text_lower = text.lower()\n",
    "print(text_lower)\n",
    "\n",
    "# Task 2: Remove punctuation and special characters\n",
    "import re\n",
    "\n",
    "# Remove punctuation and special characters\n",
    "text_cleaned = re.sub(r'[^\\w\\s]', '', text_lower)\n",
    "print(text_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Text: Hello! How is it going? I am excited.\n",
      "Final Cleaned Text: hello how is it going i am excited\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello! How's it going? I'm excited.\"\n",
    "\n",
    "# Expand contractions\n",
    "text_expanded = contractions.fix(text)\n",
    "print(\"Expanded Text:\", text_expanded)\n",
    "\n",
    "# Convert to lowercase\n",
    "text_lower = text_expanded.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "text_cleaned = re.sub(r'[^\\w\\s]', '', text_lower)\n",
    "\n",
    "print(\"Final Cleaned Text:\", text_cleaned)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
